---
title: "Abnormal stats"
author: "Brian Holt"
date: "`r Sys.Date()`"
output: slidy_presentation 
  # html_document:
  #   toc: true
  #   toc_float: true
bibliography: Abnormal_stat_references.bib
biblio-style: apalike
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F,message = F,warning = F,fig.dim=c(4,3))
library(tidyverse)
library(tinytex)
library(kableExtra)
library(dagitty)
library(ggdag)


```
# Key terms

* Independent Variable
* Dependent Variable
* Measures of Central Tendency
* Measures of Variation
* Histogram
* Null Hypothesis
* Alpha Level
* P-value
* Effect Sizes

# Independent variable aka 'x' variable {#iv}

This is often considered the variable that influences, or causes, changes in the 'y' variable. This variable usually goes on the x-axis.

Consider some height and weight data. Obviously there is a biological process that contributes to both of a person's weight and height, so it's somewhat arbitrary here.

The dots come from an individual's report of both their height and weight. 

```{r calc SD,echo=FALSE}
#HeightWeight_Data <- read.csv(file="http://faculty.seattlecolleges.edu/faculty/brian.holt/psych217/data/Student_Height_Weight_EngGpa_1-16-2008.csv",header=T)[-c(1)] 

#write.csv(HeightWeight_Data,file='height_gpa.csv')
HeightWeight_Data <- read.csv(file="\\\\S-N-NSCHOME\\nschomes\\U980354189\\Documents\\My Data Sources\\abpsych\\height_gpa.csv")[-c(1)] 
sd <- HeightWeight_Data
plot(sd$Weight,sd$Height,pch=18,xlab="weight in pounds",ylab="Height in inches",main="college student weight and heights")

```

# Dependent variable, outcome variable, aka 'y' variable {#dv}
::: columns
:::: {.column width=52%}
This is often considered the outcome variable because its value depends on the x value. Below in this graph, the dependent variable is body fat, and we might say that running _causes_ body fat to go down.


```{r,fig.dim=c(6,1)}

layout <- data.frame('x'=seq(0:1),'y'=seq(1,1),
                     'name'= c('Miles_Jogged','Body_Fat'))
miles_jogged_dag <- dagify(Body_Fat ~ Miles_Jogged,
       labels = c("Body_Fat" = 'Body_Fat', 
                  "Miles_Jogged" = "Miles_Jogged"),
       exposure = "Miles_Jogged",
       outcome = "Body_Fat",
       coords = layout )

ggdag_dseparated(from='Miles_Jogged', to='Body_Fat', miles_jogged_dag, text = FALSE)+
  geom_dag_text(vjust=3)+
  guides(fill = "none", color = "none") +  
  theme(axis.title=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        panel.background = element_blank(),
        legend.position = 'none')

```

::::
:::: {.column width=38%}
```{r,fig.dim=c(4,3),echo=F}

sigma<-rbind(c(1,-0.8,-0.7), c(-0.8,1, 0.9), c(-0.7,0.9,1))
# create the mean vector
mu<-c(10, 5, 2) 
# generate the multivariate normal distribution
df<-as.data.frame(MASS::mvrnorm(n=1000, mu=mu, Sigma=sigma))

ggplot(data=df,aes(V1,V2))+
  geom_smooth(method='lm',se=F)+
  geom_point(data=df,aes(V1,V2),alpha=.1,size=.1)+
  xlab('weekly miles jogged\n the "X" axis')+
  ylab('body fat\n the "Y" axis')+
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.title.y=element_text(angle=45,vjust=.5))
```

In the graph above, I've created extremely fake data to show a very strong effect. 

So, assuming everything else equal, the more you run, the more your body fat will decrease.
::::
:::

# Presenting and thinking about data
::: columns

:::: {.column width=45%}
Most are used to seeing spreadsheets that look like this:

```{r}
sex.d <- readRDS(file = '../HumanSex/d_Sex-survey.RDS')
sex.d %>% 
  select(id,Age,sex,q.num,score) %>% 
  pivot_wider(id_cols = c(id,Age,sex),names_from = q.num,values_from=score) %>% 
  select(-id) %>% 
  head() %>% kbl() %>% kable_classic(full_width=F)
```
::::

:::: {.column width=8%}
::::

:::: {.column width=38%}
This is just a glance of data to make this table. It comes from the Sexuality Importance scale. The scores don't really have meaning yet, but if we add them all up, we'd get a "total Score" that is claimed to represent how important sex is to that person. 
::::

:::
# Likert scale data

In this case, the numbers at the top question numbers. The survey has 17 questions. It's called a likert scale where we have people provide some level of agreement/disagreement to the questions.  And in this case, there are 7 possible choices ranging between Strongly Disagree to Strongly Agree.

We then, for better or worse, treat those answers as if they were a number.  We'll come back to how that can be dangerous later. For now, let's just see what some of these data look like. 

A scatter plot is often a way to show how these data may relate to each other.  

# Scatterplot
::: columns
:::: {.column width=38%}

```{r}
sex.d %>% 
  group_by(id,Age) %>% filter(Age<49) %>% 
  summarise(S=sum(score)) %>% ggplot(aes(x=Age,y=S))+geom_point()+
  ggtitle("Sexual Importance total score and Age")
```

If we have some understanding about the variables, maybe that one influences the other, we can plot accordingly.

::::

:::: {.column width=38%}
In this case, it doesn't make sense to say that some score on a survey is going to influence someone's age. 
So, we put Age on the x-axis, and their score on the y-axis.

This doesn't mean that X is causing Y...it's just a convention of drawing graphs. 
::::

:::

# Measure of Central Tendency {#central}

When we deal with studying groups of people it's useful, sometimes to think about the data produced by some process as having a center of gravity. in fact a lot of the statistical techniques and statistical tests have embedded in their history and appreciation for the center of mass. when you get into more advanced probability courses you'll see things like "Probability Mass distribution ",  anyway, here are some examples of ways to get to the center of that Mass.

::: columns
:::: {.column width=25%}
**Mean**: the arithmetic average where you sum all of the data points and divide by the total number of data points
::::
:::: {.column width=39%}
**Median**: the middle score. Visually, it's found by sorting all of the data and picking the data point where equal numbers update exist on either side. It is also known as the 50th percentile, which implies you could calculate any Percentile. For example the 25th percentile would mean that 25% of the scores are below that point while 75% is above.
::::

:::: {.column width=33%}
**Mode**: the most common score. It is not uncommon to have more than one mode where we might call something by modal or trimodal as the case may be.
::::
:::

# Examples of central tendency
::: columns

:::: {.column width=35%}

The mean is where you sum up all the scores $\frac{\sum{x}}{N}$.

```{r}
mean_age <- sex.d %>% select(id,Age,sex) %>%  distinct() %>% filter(Age<49) %>% 
  summarise(mean=mean(Age))

round(mean_age,1) %>% kbl() %>% kable_styling(full_width = F,position = 'center')

Age <- 
sex.d %>% select(id,Age) %>%  distinct() %>% select(Age) %>%
  filter(Age!=49) %>% 
  arrange(Age) %>% pull() 
```
It might help to see it graphically...if we make a histogram, you can sort of see how all the numbers stack together and so the mean should be in the middle of these stacks
::::

:::: {.column width=65%}

```{r}
 sex.d %>% select(id,Age,sex) %>%  distinct() %>% 
  filter(Age<49) %>% 
   ggplot(aes(x=Age))+geom_histogram(bins=35)+
  geom_vline(xintercept = mean_age[[1]],color='blue')

```

If you see that blue line as the center of mass, you can maybe see that the mass of the columns on the left is the same as what's on the right. 
It's not quite correct, but if you add up all the ages on the left side, you get about `r sum(Age[Age<mean_age[[1]]])`, and `r sum(Age[Age>mean_age[[1]]] )` on the right side.

::::
:::
# Median Example

```{r,fig.dim=c(12,5)}
sex.age.median <- 
  sex.d %>% select(id,Age) %>%  distinct() %>% filter(Age<49) %>% 
    summarise(q=quantile(Age),
              median=median(Age),
              n=n()) %>% 
  mutate(x=q,
         y=0,
         xend=q,
         yend=.3)

# 
# if(length(Age) %% 2 == 0) {
#   pos_med_even <- mean(c(length(Age)/2,(length(Age)/2)+1))
# 
# } else {
#   pos_med_odd <- (length(Age)+1)/2
# 
# }  

find_median <- function(v) {
  v_sorted <- sort(v)
  n <- length(v_sorted)
  if(n %% 2 == 0) {
    med_even <- mean(c(n/2, (n/2)+1))
    return(med_even)
  } else {
    med_odd <- (n+1)/2
    return(med_odd)
  }
}

med_pos<- find_median(Age)

sex.d %>% select(id,Age) %>%  distinct() %>% filter(Age!=49) %>%   arrange(Age) %>% select(Age) %>% 
    mutate(position=row_number()) %>% 
  ggplot(aes(x=position,y=0))+geom_line(linewidth=.1)+
  geom_label(aes(label=Age),vjust=-1,size=3)+
  geom_text(aes(label=position),vjust=1.5,size=4)+
  theme(axis.line.y=element_blank(),
          axis.text.y=element_blank(),
          axis.ticks.y=element_blank(),
          axis.title.y=element_blank(),
          axis.text.x=element_blank(),
          axis.ticks.x=element_blank(),
          axis.title.x=element_blank(),
        panel.spacing = unit(.1, "lines"))+
  ylim(-.05,.08)+
  geom_segment(aes(x =med_pos, 
                   y=0,
                   xend=med_pos,
                   yend=.05),
               color='blue')+
  geom_text(aes(label="Ages in order from least\nto greatest"),x=10,y=.05)+
  geom_text(aes(label="There are equal number\nof scores\non each side of the blue line"),x=38,y=.05)+
  geom_text(aes(label="Order and position of the Ages,\nstarting at position 1"),x=15,y=-.03,size=4)+
  geom_text(aes(label=paste0("So, ",sex.age.median$q[[3]]," should be the median,\nor midpoint of the scores"),x=35,y=-.03))

```


# Mode Example

```{r}
a<- sex.d %>% select(id,Age) %>%  distinct() %>% group_by(Age) %>% 
  summarise(count=n()) %>% arrange(desc(count)) 

a %>% 
  filter(count>1) %>% 
  kbl() %>% kable_styling(full_width = F,font_size=12)
```

The table above shows the count of each Age reported with counts of one removed to save space.  Obviously, the Age `r a$Age[1]` is the most common Age, so it's the mode.


# Putting them all together
::: columns

:::: {.column width=25%}
So, maybe putting them on the same graph might help.

```{r}
as.data.frame(Age) %>% 
  ggplot(aes(x=Age))+geom_histogram(bins=35)+
  geom_vline(xintercept=mean(Age),color='orange')+
  geom_vline(xintercept=median(Age),color='blue')+
  geom_vline(xintercept=a$Age[1],color='purple')+
  ggtitle("Purple = mode\nBlue = median\nOrange = mean")
```
::::

:::: {.column width=10%}

:::: 

:::: {.column width=65%}
If we did it with more 'normally distributed data', it would look like this:

```{r}
b<- sd %>% select(Height) %>%  group_by(Height) %>% 
  mutate(Height=round(Height,1)) %>% 
  summarise(count=n()) %>% arrange(desc(count))  

sd %>% 
  ggplot(aes(x=Height))+geom_histogram(binwidth=1)+
  geom_vline(xintercept=mean(sd$Height),color='orange')+
  geom_vline(xintercept=median(sd$Height),color='blue')+
  ggtitle("Omitting the mode\nbecause there are several\nBlue = median\nOrange = mean")+
  theme(axis.text.x = element_text(angle = 45)) +
   scale_x_continuous(breaks = seq(50,80, by = 1))


# sd %>% 
#   mutate(Sex = tolower(Sex),
#          Sex = case_when(Sex == 'female'~ 'f',
#                    Sex == 'male'~ 'm',
#                    T~Sex)) %>% 
#   group_by(Sex) %>% 
#   ggplot(aes(x=Height))+geom_histogram(binwidth=1)+
#   geom_vline(xintercept=mean(sd$Height),color='orange')+
#   geom_vline(xintercept=median(sd$Height),color='blue')+
#   ggtitle("Omitting the mode\nbecause there are several\nBlue = median\nOrange = mean")+
#   theme(axis.text.x = element_text(angle = 45)) +
#    scale_x_continuous(breaks = seq(50,80, by = 1))+
#   facet_wrap(~Sex)
# 
# 
# 
# sd %>%
#   mutate(Sex = tolower(Sex),
#          Sex = case_when(Sex == 'female'~ 'f',
#                    Sex == 'male'~ 'm',
#                    T~Sex)) %>%
#   group_by(Sex,Height) %>%
#     summarise(count=n(Height))
#             
#             
            # 
            # m=mean(Age),
            # median(Age))


```

Usually, if all 3 measures are the same, we often think the data may be normally distributed.  This particular data set appears normally distributed; the Age data don't, which make sense given it was collected from community college students. 

::::

:::

# Measure of Variation {#variation}

Measures of **variation** attempt to summarize the degree to which data hang around this measure of central tendency. 

The most common measure of central tendency  is the mean, and if you consider that average as type of gravity, then variation is an attempt to try to describe how data points orbit that average.

You can think of Saturn as the center of gravity, the arithmetic mean, and that the rings of Saturn are made up of data points (asteroids). 

If the data points are really close around the average then you would have a small amount of variation; if the data points are very spread out then you expect a lot of variation.

**Standard Deviation**: this is the average amount of variation. Small numbers mean that there is very little variation while large numbers imply lots of variation.

# Histograms

A histogram is a visual representation of counting data points. They are also known as frequency distributions. So, it's not surprising that frequency count of data is its main purpose.

[You should watch this video which is a nice gentle introduction to histograms.](https://youtu.be/qBigTkBLU6g?t=17){target='_blank'} [@statquestwithjoshstarmerStatQuestHistogramsClearly2017a]

```{r,echo=F,message=F}
library(psych)
```
# Example measures of central tendency

The data below is a data set curated by the Personality Project [-@PersonalityProject]. 

It represents `r length(bfi$age)` participants in study on personality. You may remember a few weeks ago I introduced the big five model of personality. 

The major five components include openness, conscientiousness, extroversion, agreeableness, neuroticism. Here are some brief stats on the demographic information for these participants:

```{r,echo=F}
summary(bfi[,26:28])
```
# Example of variation

And the standard deviation (a measure of spread around the center of gravity): 

 variable || value
 ---|---|---
 Sex||`r sd(bfi[,26])`
 education||`r sd(bfi[,27],na.rm=T)`
 age||`r sd(bfi[,28])`

So, for example, age varies around the mean (`r round(mean(bfi$age,na.rm=T),3)`) about `r round(sd(bfi[,28]),3)` years (i.e., the data are mostly between $\pm$ `r round(sd(bfi[,28]),3)` of `r round(mean(bfi$age,na.rm=T),3)`)

# Example histogram

So, let's look at the histogram of age:

```{r,fig.dim=c(4,3),echo=F}
hist(bfi$age)
```

Not surprisingly, as most research published in psychology comes from college samples Looks like the most common score is going to be around 20. But you do notice that there's some range that goes up into the 80’s even.

# A word about normal distributions

Many of you are familiar with the bell-shaped curve that comes from probability distribution most commonly known as the normal distribution.

This particular data set of age is not normally distributed.

Imagine that I took a random sample of 35 people. And I took the average of that sample. It might look like this:

```{r,echo=F}
(sample<- sample(bfi$age,35,replace = T))
(mean(sample))
```
Let's do this 20 times

#  Several samples

```{r, small n histogram samples,echo=F,fig.dim=c(6,4),message=F,warning=F}
sample_count=20
n=35
samps<- rep(1:sample_count,each=n)
sample.1 <- sample(bfi$age, n*sample_count)
sample.20<- matrix(c(sample.1,samps),nrow = n*sample_count,ncol = 2,byrow = F)
colnames(sample.20) <- c('value','group')
as.data.frame(sample.20) %>% ggplot(aes(value))+geom_histogram()+facet_wrap(~group)
```

Imagine we did this a thousand times but instead of showing individual histograms, we just took the average of each sample, and then treated it as a data point?  We would have thousands of averages.  I'll do that 20 times

# twenty averages

```{r, small n sample means,echo=F,fig.dim=c(6,4),message=F,warning=F}
sample_count=20
n=35
(sapply(1:sample_count,function(x) mean(sample(bfi$age,x,replace=T))))

```

These are averages.

We took 20 samples of 35 people and listed them.  

Now let's do that, but 100 times.  I won't show you the individual averages but instead will make  a histogram. 

#  histogram of a sampling distribution

```{r, 100 sample means,echo=F,fig.dim=c(3,3),message=F,warning=F}
sample_count=100
n=35
out<- sapply(1:sample_count,function(x) mean(sample(bfi$age,x,replace=T)))
hist(out,breaks=10)
abline(v=mean(out),col='blue')
abline(v=mean(bfi$age),col='red')
```

# Let's do it again but with 1000 samples

```{r, 1000 sample means,echo=F,fig.dim=c(3,3),message=F,warning=F}
sample_count=1000
n=35
out<- sapply(1:sample_count,function(x) mean(sample(bfi$age,x,replace=T)))
hist(out,breaks=30)
abline(v=mean(out),col='blue')
abline(v=mean(bfi$age),col='red')
```


Notice that it's looking more normal.

The key about this is that when we do a real experiment, it's like we are doing one of these little tiny samples *and* if we were to do another sample and another, took averages of each, we'd be getting closer to the 'true' average.

Also, many of you are familiar with the bell shape.  It's just this histogram with infinitesimal width on the vertical rectangles.

# smaller rectangles

```{r,echo=F}
par(mfrow=c(2,3))
hist(out,breaks=10)
hist(out,breaks=20)
hist(out,breaks=30)
hist(out,breaks=50)
hist(out,breaks=100)
```

This is the same data but with more 'bins'  They are getting smaller and smaller and so the jaggedness of the histogram is smoothing.  

And all of this is actually about probability...the area under the curve is just counting the area of the rectangle divided by all the rectangles. 

# Null Hypothesis

  Broadly speaking the null hypothesis is a statement or prediction that 'X' will not cause changes in 'Y'.  This can show up in various forms, a simple one is where two groups, a control group and one that receives an experimental medication, are believed to have the same outcomes--that the medication won't work. 
  
You may want to investigate why the null is the baseline prediction

# [Alpha](https://en.wikipedia.org/wiki/Significance_level)

  This is the number of times you are willing to be wrong when doing an experiment.  
  
  It's more complicated than this, but if you imagine doing an experiment, where the differences between groups may not be obvious, you should be transparent about your threshold for being wrong.  In other words, if I perform the same experiment over and over, how many times will I find a 'real' effect, and how many times will I find a "chance" effect?
  
In Psychology, the tradition is 5%. There is history to this that is worthy of letting go, but it's nonetheless a common threshold. It is saying that if you did the experiment over and over, say 100 times, that 5 times out of a 100 we'd find an effect that was just random and therefore **not real, or chance.** 

You can't know for sure, so you say that 5 out of 100 times you would be willing to be wrong.  By the way, this definition I've given you is just a part of what Alpha is.  I have given you basically the [type 1 error](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors#Type_I_error) rate.

# p-value

If the null hypothesis is true, the p-value is the probability that the found effect, or one larger, is real.  It's saying that when you find a difference between groups, that the chance of finding that difference (or one larger) has a certain probability, or a p-value---assuming that the null is true.  

For example, let's say I do a study seeing the effect of regular dancing(IV) has on life satisfaction (DV).   Let's say I have 2 groups, one group does no dancing, like no physical activity during the week while the experimental group does 1 hour of dancing for 5 days for the week. 

Let's say I find that the average life satisfaction reports (the DV) are different between the two groups.  Say, control =2/5; dance = 4/5, after performing the appropriate test (t-test, most likely) finding the p-value is .03.  What does that mean?

# P value .03

If it is the case that the null hypothesis, which means there's no difference between the two groups, then the probability of finding a difference between the two groups (2 and 4) would be about 3x out of a 100 of such experiments. 

In other words, if we did this exact same experiment 100 times and there is no  Effect of dancing on life satisfaction, we would still find chance differences of 2 (4 - 2) and larger, about 3% of the time.

 It's a mouthful.

# Effect Sizes

These are statistical efforts at comparing outcomes across different studies.  By taking the experimental properties (sample sizes, demographics, methods), researchers will conduct a "meta-analysis" that attempts to compare similar studies and their outcomes to see if you can organize, or sort, factors of influence.  

For example, in physics, on our planet gravity has the largest effect size over the speed that an object falls from a specific height.  Friction from the air also impacts the speed of decent but it has a smaller "effect".  

# Meta analyses

These use studies as data points. Instead of calculating an average like we just did with the age of the personality participants, you calculate a different statistic for a study. You put that statistic into a bin, and then you do it again with a different study and you do it over and over until you get several studies so that you can get the average of this statistic.

# common effect size statistic

What is the statistic? There are few different versions of it but the classic example is what's known as Cohen's d.  It's basically just an average between two groups. So in our example above, we had an average of 2 versus an average of 4. So the effect size in this particular study would be 2/(pooled standard deviation).  If you want to know more, you'll want to look this up.

Dividing by that pooled standard deviation is a way to standardize the difference between groups.  This is how we can compare across different studies. 

And that is what Cohen's d does.  As a general rule you interpret and effect size like this

d = 0.20 indicates a small effect,

d = 0.50 indicates a medium effect and

d = 0.80 indicates a large effect.


# Intro to stats for abnormal psych students

Big idea about research is that it is incredibly rare to perform one study/experiment and to have it change the known literature.  Science is 99.99% incremental.

So, when you learn about a topic via reading journal articles (experiments) be humble about what you find.

Also, remember the 3 main parts of John Stuart Mills thoughts on Cause and effect.  To establish that you need:

1. Co-variance between variables (correlation)

2. Temporal precedence (the cause has to come before the effect)

3. All other explanations must be ruled out.

   The last one, 3, is the hard part.  


# Stat book contents

This is a skeletal outline of most intro stat books:

    What are numbers
    How to graph
    Probability
    Probability Distribution broadly
    Probability Distributions, specifically the Normal, central limit theorem
    Z-scores
    T-tests
    Correlations
    ANOVA
    Linear Regression
    Chi-Square
    Effect Sizes

In abnormal psychology you see a lot of linear regressions.  You also see a Effect Sizes.

To have an introductory understanding of regression, you should understand probability and Probability Distributions. 

So, let's jump into probability distributions by first looking at some graphs.

# Graphing with a histogram

Histograms are useful for seeing how small sets of data "look".  It often is useful to do this for seeing where your data exists.

Imagine we wanted to discover how tall people are.  Let us say I can sample 100 people, randomly, from Seattle.  Here they are listed in inches

```{r height data,echo=F}
h <- round(rnorm(100,mean=66, sd=1),1)
print(h)
```

We can see this data more easily if we graph it.  There are many ways to, but this is a classic way: The Histogram

# another histogram

```{r histogram,echo=F,fig.dim=c(4,4)}

hist(h,xlab = 'Height\n blue line is the average',main = 'histogram of heights in Seattle')
abline(v=mean(h),col='blue')

```

What you should see here is that the vertical 'y' axis is a frequency, a count, while the 'x' axis is the range of scores.  So, from 64 to 64.5 inches tall, there appears to be about `r length(h[h>=64&h<=64.5])` people.  If you look at our original data, you should see this is true:

# sorted heights

```{r sort data,echo=F}
sort(h)
```

And here is the data pulled out so you aren't overtaxing your eyes: `r sort(h[h>=64&h<=64.5])`

**But what I want you to really see is the shape of this histogram.**  It is beginning to look like a bell curve, the classic normal distribution.  If we could sample a bit larger number of people, say 200, you would a shape more close to this normal curve:

```{r More data for histogram,echo=F,fig.dim=c(3,3)}
h.1 <- rnorm(200, mean=66)
hist(h.1,xlab = 'Height',main = 'histogram of 200 heights in Seattle')
```

# And what about 1000 people? 

```{r  1000 data for histogram,echo=F,fig.dim=c(5,5)}
h.1000 <- rnorm(1000, mean=66)
par(mfrow=c(1,2))
hist(h.1000,xlab = 'Height',main = 'histogram of 1000 heights in Seattle')
plot(density(h.1000),xlab="heights",main="Distribution")
```

I hope you see where this is going.  Because then the following makes some sense:

The area under the curve on the right is the probability of those values on the x axis occurring. 

>To understand why this is, you'll want to take calculus (at least I and II) to understand the Fundamental Theorem of Calculus.

# area under curve or in the bins

Back to stats, and for example, look **back** at the histogram of 100 people.  We can use the the number of people between 64 and 64.5 inches.  In this case there are `r length(h[h>=64&h<=64.5])`, and since there are 100 people, you can do the quick calculation that `r length(h[h>=64&h<=64.5])` out of 100 is `r length(h[h>=64&h<=64.5])/100`.  If you were to ask the question "what is the chance that someone is between 64 and 64.5 inches tall in our sample", you would say about `r length(h[h>=64&h<=64.5])`% .

The point here is that if you know something about a sample's distribution, you can start making some good guesses about the larger population.  And, this is the cornerstone for [inferential stats](https://en.wikipedia.org/wiki/Statistical_inference), and p-values are suppose to guide us in this work.

# Big leap to correlations

I'm going to load a data set that has 231 cases, people, where they have given data about their personality.  I've no idea about who these people are, though I imagine there is some information online.  You can probably recognize some of these variables below..

# additional ttest example to demonstrate the dancing pvale

```{r}
n <- 30
set.seed(1)
(d <- rnorm(n,4,1))
d <- unlist(lapply(d,function(x) ifelse(x>5,x <- 5,x)))
set.seed(1)
(nd <- rnorm(n,2.5,1))
nd <- unlist(lapply(nd,function(x) ifelse(x<0,x*-1,x)))

t.test(d,nd)

plot(density(d),xlim=c(-2,8))
lines(density(nd),col='blue')
abline(v=mean(d))
abline(v=mean(nd),col='blue')
```
# References